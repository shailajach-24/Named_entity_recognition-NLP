{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "Hi 🙂, if you are seeing this notebook, you have succesfully started your first project on FloydHub 🚀, hooray!!\n",
    "\n",
    "[Named Enity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) is one of the most common [NLP](https://en.wikipedia.org/wiki/Natural-language_processing) problems. The goal is classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "*What can you use it for?* Here are a few ideas - social media, chatbot, customer support tickets, survey responses, and data mining! \n",
    "\n",
    "### Predicting named entities of GMB(Groningen Meaning Bank) corpus\n",
    "\n",
    "In this notebook we will perform a [Sequence Tagging with a LSTM-CRF model](https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/) to extract the named entities from the annotated corpus.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/named-entity-recognition-template/master/images/ner-image.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "\n",
    "Entity tags are encoded using a BIO annotation scheme, where each entity label is prefixed with either B or I letter. B- denotes the beginning and I- inside of an entity. The prefixes are used to detect multiword entities, e.g. sentence:\"World War II\", tags:(B-eve, I-eve, I-eve). All other words, which don’t refer to entities of interest, are labeled with the O tag.\n",
    "\n",
    "Tag | Label meaning | Example Given\n",
    "------------ | ------------- | \n",
    "geo | Geographical Entity | London\n",
    "org | Organization | ONU\n",
    "per | Person | Bush\n",
    "gpe | Geopolitical Entity | British\n",
    "tim | Time indicator | Wednesday\n",
    "art | Artifact | Chrysler\n",
    "eve | Event | Christmas\n",
    "nat | Natural Phenomenon | Hurricane\n",
    "O | No-Label | the\n",
    "\n",
    "We will:\n",
    "- Preprocess text data for NLP\n",
    "- Build and train a Bi-directional LSTM-CRF model using Keras and Tensorflow\n",
    "- Evaluate our model on the test set\n",
    "- Run the model on your own sentences!\n",
    "\n",
    "#### Instructions\n",
    "- To execute a code cell, click on the cell and press `Shift + Enter` (shortcut for Run).\n",
    "- To learn more about Workspaces, check out the [Getting Started Notebook](get_started_workspace.ipynb).\n",
    "- **Tip**: *Feel free to try this Notebook with your own data and on your own super awesome named entity recognition task.*\n",
    "\n",
    "Now, let's get started! 🚀\n",
    "\n",
    "### Initial Setup\n",
    "Let's start by importing some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install extra-dependencies\n",
    "! pip -q install git+https://www.github.com/keras-team/keras-contrib.git sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run 🙂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams if GPU is available\n",
    "#if tf.test.is_gpu_available():\n",
    "    #BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "  #  EPOCHS = 5  # Number of passes through entire dataset\n",
    "   # MAX_LEN = 75  # Max length of review (in words)\n",
    "    #EMBEDDING = 40  # Dimension of word embedding vector\n",
    "\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "#else:\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "MAX_LEN = 75\n",
    "EMBEDDING = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The movie ner dataset is already attached to your workspace (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n",
    "\n",
    "Let's take a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  47959\n",
      "Number of words in the dataset:  35178\n",
      "Tags: ['B-org', 'O', 'I-gpe', 'B-gpe', 'I-tim', 'I-org', 'B-nat', 'I-nat', 'B-geo', 'B-per', 'I-art', 'I-eve', 'I-geo', 'B-tim', 'B-art', 'I-per', 'B-eve']\n",
      "Number of Labels:  17\n",
      "What the dataset looks like:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1  Sentence: 1             of   IN      O\n",
       "2  Sentence: 1  demonstrators  NNS      O\n",
       "3  Sentence: 1           have  VBP      O\n",
       "4  Sentence: 1        marched  VBN      O\n",
       "5  Sentence: 1        through   IN      O\n",
       "6  Sentence: 1         London  NNP  B-geo\n",
       "7  Sentence: 1             to   TO      O\n",
       "8  Sentence: 1        protest   VB      O\n",
       "9  Sentence: 1            the   DT      O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "print(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n",
    "\n",
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words)\n",
    "print(\"Number of words in the dataset: \", n_words)\n",
    "\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "print(\"Tags:\", tags)\n",
    "n_tags = len(tags)\n",
    "print(\"Number of Labels: \", n_tags)\n",
    "\n",
    "print(\"What the dataset looks like:\")\n",
    "# Show the first 10 rows\n",
    "data.head(n=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what a sentence looks like:\n",
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "    \"\"\"Class to Get the sentence in this format:\n",
    "    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]\"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Args:\n",
    "            data is the pandas.DataFrame which contains the above dataset\"\"\"\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        \"\"\"Return one sentence\"\"\"\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "print('This is what a sentence looks like:')\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output Cell above, each sentence in the dataset is represented as a list of tuple: [`(Token_1, PoS_1, Tag_1)`, ..., `(Token_n, PoS_n, Tag_n)`]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all the sentences\n",
    "sentences = getter.sentences\n",
    "\n",
    "# Plot sentence by lenght\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.title('Token per sentence')\n",
    "plt.xlabel('Len (number of token)')\n",
    "plt.ylabel('# samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before feeding the data into the model, we have to preprocess the text.\n",
    "\n",
    "- We will use the `word2idx` dictionary to convert each word to a corresponding integer ID and the `tag2idx` to do the same for the labels. Representing words as integers saves a lot of memory!\n",
    "- In order to feed the text into our Bi-LSTM-CRF, all texts should be the same length. We ensure this using the `sequence.pad_sequences()` method and `MAX_LEN` variable. All texts longer than `MAX_LEN` are truncated and shorter texts are padded to get them to the same length.\n",
    "\n",
    "The *Tokens per sentence* plot (see above) is useful for setting the `MAX_LEN` training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Obama is identified by the index: 7879\n",
      "The labels B-geo(which defines Geopraphical Enitities) is identified by the index: 9\n",
      "Raw Sample:  Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Raw Label:  O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "After processing, sample: [ 1549 22421 22559 16294  6874 11916 16119  3112   748 27625  4703  5206\n",
      " 10175 16120 23779 27625 27373 22421 19911 23650   349 23554  4836  4142\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "After processing, labels: [[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Key:word -> Value:token_index\n",
    "# The first 2 entries are reserved for PAD and UNK\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "print(\"The word Obama is identified by the index: {}\".format(word2idx[\"Obama\"]))\n",
    "print(\"The labels B-geo(which defines Geopraphical Enitities) is identified by the index: {}\".format(tag2idx[\"B-geo\"]))\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Convert each sentence from list of Token to list of word_index\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# Convert Tag/Label to tag_index\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "# One-Hot encode\n",
    "y = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n",
    "\n",
    "print('Raw Sample: ', ' '.join([w[0] for w in sentences[0]]))\n",
    "print('Raw Label: ', ' '.join([w[2] for w in sentences[0]]))\n",
    "print('After processing, sample:', X[0])\n",
    "print('After processing, labels:', y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will implement a model similar to Zhiheng Huang’s [Bidirectional LSTM-CRF Models for Sequence Tagging](https://arxiv.org/pdf/1508.01991v1.pdf).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/named-entity-recognition-template/master/images/bilstm-crf.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "*Image from [the paper](https://arxiv.org/pdf/1508.01991v1.pdf)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "# Model definition\n",
    "input = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN)(input)  # default: 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluate\n",
    "\n",
    "The Training is defined at the beginning by the type of instance on which runs:\n",
    "\n",
    "- On CPU machine: 25 minutes for 5 epochs.\n",
    "- On GPU machine: 5 minute for 5 epochs.\n",
    "\n",
    "*Note*: Accuracy isn't the best metric to choose for evaluating this type of task because most of the time it will correctly predict '**O**' or '**PAD**' without identifing the important Tags, which are the ones we are interested in. So after training for some epochs, we can monitor the **precision**, **recall** and **f1-score** for each of the Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_tr, np.array(y_tr), batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "pred_cat = model.predict(X_te)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "y_te_true = np.argmax(y_te, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Convert the index to tag\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "y_te_true_tag = [[idx2tag[i] for i in row] for row in y_te_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=y_te_true_tag)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate some samples in the test set. (At each execution it will test on a different sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0,X_te.shape[0]) # choose a random number between 0 and len(X_te)\n",
    "p = model.predict(np.array([X_te[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_te[i], -1)\n",
    "\n",
    "print(\"Sample number {} of {} (Test Set)\".format(i, X_te.shape[0]))\n",
    "# Visualization\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_te[i], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-2], idx2tag[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn\n",
    "\n",
    "Test out the model you just trained. Run the code Cell below and type your reviews in the widget, Have fun!🎉\n",
    "\n",
    "Here are some inspirations:\n",
    "\n",
    "- Obama was the president of USA.\n",
    "- The 1906 San Francisco earthquake was the biggest earthquake that has ever hit San Francisco on April 18, 1906\n",
    "- Next Monday is Christmas!\n",
    "\n",
    "Can you do better? Play around with the model hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Custom Tokenizer\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "    \n",
    "def get_prediction(sentence):\n",
    "    test_sentence = tokenize(sentence) # Tokenization\n",
    "    # Preprocessing\n",
    "    x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]],\n",
    "                            padding=\"post\", value=word2idx[\"PAD\"], maxlen=MAX_LEN)\n",
    "    # Evaluation\n",
    "    p = model.predict(np.array([x_test_sent[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    # Visualization\n",
    "    print(\"{:15}||{}\".format(\"Word\", \"Prediction\"))\n",
    "    print(30 * \"=\")\n",
    "    for w, pred in zip(test_sentence, p[0]):\n",
    "        print(\"{:15}: {:5}\".format(w, idx2tag[pred]))\n",
    "\n",
    "interact_manual(get_prediction, sentence=widgets.Textarea(placeholder='Type your sentence here'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving Vocab\n",
    "with open('models/word_to_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    " \n",
    "# Saving Vocab\n",
    "with open('models/tag_to_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(tag2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Saving Model Weight\n",
    "model.save_weights('models/lstm_crf_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's all folks - don't forget to shutdown your workspace once you're done 🙂"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
